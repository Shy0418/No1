{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#uie大一统模型\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from  paddlenlp import Taskflow\n",
    "schema=[\n",
    "    '人物',\n",
    "    '组织',\n",
    "    '时间',\n",
    "    '开始位置',\n",
    "    '关系'\n",
    "]\n",
    "sentence = '郭艾伦今年28岁，2.10米高，体重200公斤，是名篮球运动员，他患有糖尿病，郭艾伦表示有CBA球队联系自己# 8月20日，@郭艾伦 现身媒体人@刘宝杰聊篮球 评论区：“你之前发的没有cba俱乐部联系过我的新闻也全是假的 如果不是有联系好我提出转会干啥？ 撒谎也不动动脑子！，在8月10日，8月11日这两天，,谁跟我联没联系过谈什么了你知道？ 你是我经纪人？现在又拿个日本联赛出来想恶心我一下然后蹭流量还不承认 这种连我都本人都不知道的事情还在嘴硬 连翻译软件都不会用还装什么大聪明。”#郭艾伦说有CBA球队联系自己才提转会# 麦子哥哥QaQ的微博视频收起'\n",
    "\n",
    "ie = Taskflow('information_extraction',schema=schema)\n",
    "pprint(ie(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "path_zhihu=r'E:\\\\论文\\\\数据\\\\知乎.csv'\n",
    "path_weibo=r'E:\\\\论文\\\\数据\\\\微博all_t.csv'\n",
    "path_tieba=r'E:\\\\论文\\\\数据\\\\百度贴吧.csv'\n",
    "df = pd.read_csv(path_zhihu,encoding='utf-8',header = 0)\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_unique = df.标题.unique()\n",
    "data_unique\n",
    "pd.DataFrame(data_unique).to_csv(r'知乎问题数据集.csv',encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from paddlenlp import Taskflow\n",
    "schema = ['健康','社交','自我实践','自我表现','自主性']\n",
    "text = []\n",
    "ie = Taskflow('information_extraction',schema = schema,batch_size = 1,model='uie-medical-base',position_prob=0.5,precision='fp16')\n",
    "for i in data_unique[0:10]:\n",
    "        print(i)\n",
    "        sen = ie(i)\n",
    "        text.append(sen)\n",
    "print(text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification,AutoTokenizer,pipeline\n",
    "import pandas as pd\n",
    "model = AutoModelForTokenClassification.from_pretrained('roberta-base-finetuned-cluener2020-chinese')\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base-finetuned-cluener2020-chinese')\n",
    "ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "sentence = '江苏警方通报特斯拉冲进一家4S店铺'\n",
    "sentence = '你好，我是Michael，我是一名学生，我来自东北林业大学，信息与计算机工程学院，我家住江苏苏州张家港。'\n",
    "list = ner(sentence)\n",
    "pprint(list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "      words        D1    D2   D3   D4   D5   D6   D7   D8    Num\n0      爱一个人      Love   NaN  NaN  NaN  NaN  NaN  NaN  NaN      0\n1        表白      Love   NaN  NaN  NaN  NaN  NaN  NaN  NaN      1\n2        初恋      Love   NaN  NaN  NaN  NaN  NaN  NaN  NaN      2\n3        单身      Love   NaN  NaN  NaN  NaN  NaN  NaN  NaN      3\n4        对象      Love   NaN  NaN  NaN  NaN  NaN  NaN  NaN      4\n...     ...       ...   ...  ...  ...  ...  ...  ...  ...    ...\n12432    早*  Relative  Time  NaN  NaN  NaN  NaN  NaN  NaN  12432\n12433    周*  Relative  Time  NaN  NaN  NaN  NaN  NaN  NaN  12433\n12434    鬼*  Religion   NaN  NaN  NaN  NaN  NaN  NaN  NaN  12434\n12435    观*       See   NaN  NaN  NaN  NaN  NaN  NaN  NaN  12435\n12436    倏*      Time   NaN  NaN  NaN  NaN  NaN  NaN  NaN  12436\n\n[12437 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>words</th>\n      <th>D1</th>\n      <th>D2</th>\n      <th>D3</th>\n      <th>D4</th>\n      <th>D5</th>\n      <th>D6</th>\n      <th>D7</th>\n      <th>D8</th>\n      <th>Num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>爱一个人</td>\n      <td>Love</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>表白</td>\n      <td>Love</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>初恋</td>\n      <td>Love</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>单身</td>\n      <td>Love</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>对象</td>\n      <td>Love</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12432</th>\n      <td>早*</td>\n      <td>Relative</td>\n      <td>Time</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12432</td>\n    </tr>\n    <tr>\n      <th>12433</th>\n      <td>周*</td>\n      <td>Relative</td>\n      <td>Time</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12433</td>\n    </tr>\n    <tr>\n      <th>12434</th>\n      <td>鬼*</td>\n      <td>Religion</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12434</td>\n    </tr>\n    <tr>\n      <th>12435</th>\n      <td>观*</td>\n      <td>See</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12435</td>\n    </tr>\n    <tr>\n      <th>12436</th>\n      <td>倏*</td>\n      <td>Time</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12436</td>\n    </tr>\n  </tbody>\n</table>\n<p>12437 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import paddlenlp\n",
    "dictionary = pd.read_excel(r'E:\\\\论文\\\\数据\\\\WenXinV4.0师姐发的版本\\\\dic\\\\dictionary_all.xlsx')\n",
    "dictionary\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(dictionary).to_csv(r'词典.txt',sep=' ',encoding='utf-8',index=0,header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictionary1 = dictionary.fillna('0')\n",
    "dictionary1\n",
    "dictionary2 = dictionary1.drop(labels='Num',axis=1)\n",
    "dictionary2\n",
    "dictionary3 = dictionary2.to_dict('index')\n",
    "dictionary3\n",
    "dictionary4 = np.array(dictionary2)\n",
    "dictionary4\n",
    "dictionary_text = dict()\n",
    "for i in range(len(dictionary4)):\n",
    "    dictionary_text[str(dictionary4[i][0])]=[dictionary4[i][1],dictionary4[i][2],dictionary4[i][3],dictionary4[i][4],dictionary4[i][5],dictionary4[i][6],dictionary4[i][7],dictionary4[i][8]]\n",
    "dictionary_text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "# roberta = TransformerDocumentEmbeddings('bert-base-chinese')\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "kw_model = KeyBERT(model= sentence_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zhihu = pd.read_csv(r'知乎问题数据集.csv',encoding='utf-8',index_col=0)\n",
    "zhihu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(zhihu).to_csv(r'知乎问题数据集.txt',encoding='utf-8',header=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zhihu1 = np.array(zhihu)\n",
    "zhihu1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "jieba.load_userdict(\"dictionary.txt\")\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "doc = \"我爱北京天安门\"\n",
    "keywords = kw_model.extract_keywords(doc, vectorizer=vectorizer)\n",
    "import pandas as pd\n",
    "seed_words1 = pd.read_excel(r'E:\\论文\\数据\\WenXinV4.0师姐发的版本\\dic\\dictionary_all.xlsx',header=0,index_col=None)\n",
    "seed_words1\n",
    "seed_word = seed_words1.words\n",
    "seed_word\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_words = list(seed_word)\n",
    "seed_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(seed_words).to_csv(r'Seed_words.txt',encoding='utf-8',index=None,header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = pd.read_table(r'Seed_words.txt',encoding='utf-8',header=None)\n",
    "seed\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "zhihu_question = pd.read_table(r'知乎问题数据集.txt',encoding='utf-8',header=None,index_col=0,sep=r'\\n')\n",
    "zhihu_question"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(zhihu_question)\n",
    "# zhihu_question"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "jieba.load_userdict(\"dictionary.txt\")\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import  datetime\n",
    "kw_model = KeyBERT('all-MiniLM-L6-v2')\n",
    "i=0\n",
    "\n",
    "# for i in range(len(zhihu1)):\n",
    "path = '抽取1.txt'\n",
    "time_start = datetime.datetime.now()\n",
    "with open(path,mode = 'w+',encoding='utf-8') as file:\n",
    "    for i in range(len(zhihu1)):\n",
    "        keywords = kw_model.extract_keywords(zhihu1[i],vectorizer=vectorizer,stop_words='E:\\论文\\python代码\\停用词表\\stopwords_all.txt',keyphrase_ngram_range=[1,2],use_maxsum=False,use_mmr=False,top_n=100,seed_keywords=seed_words)\n",
    "        file.write(str(keywords))\n",
    "        file.write('\\n')\n",
    "        print(keywords)\n",
    "time_end = datetime.datetime.now()\n",
    "print(time_end-time_start)\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictionary = pd.read_csv(r'dictionary.txt',sep='\\t',header=None)\n",
    "dictionary\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.load_userdict(\"dictionary.txt\")\n",
    "word_list = jieba.cut(\"我今天不处理逾期信用贷款，因为你们中国银行的APP根本打不开\")\n",
    "print(\",\".join(word_list))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel,BertForSequenceClassification\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from distutils.spawn import find_executable\n",
    "if find_executable('latex'):\n",
    "    print('latex installed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_table(r'抽取结果.txt',sep='\\t',header=None)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data1 = np.array(data)\n",
    "data1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data1[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}